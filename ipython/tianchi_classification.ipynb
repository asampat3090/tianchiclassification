{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tianchi Notebook. Clothes classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Volumes/Macintosh HD/Users/Benze/Downloads/data/training/111.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/training/155.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/training/160.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/training/228.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/training/284.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/training/368.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/training/48.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/training/505.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/training/52.pickle'] ['/Volumes/Macintosh HD/Users/Benze/Downloads/data/test/111.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/test/155.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/test/160.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/test/228.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/test/284.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/test/368.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/test/48.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/test/505.pickle', '/Volumes/Macintosh HD/Users/Benze/Downloads/data/test/52.pickle']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '/Volumes/Macintosh HD/Users/Benze/Downloads/data'\n",
    "train_data_path = \"/\".join([data_dir, 'training'])\n",
    "test_data_path = \"/\".join([data_dir, 'test'])\n",
    "\n",
    "# train ~ 6Gb and test ~ 1.2 Gb\n",
    "train_filenames = [\"/\".join([train_data_path, f]) for f in os.listdir(train_data_path)]\n",
    "test_filenames = [\"/\".join([test_data_path, f]) for f in os.listdir(test_data_path)]\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "nb_total_train_img = 90000\n",
    "nb_total_test_img = 18000\n",
    "\n",
    "reduction = 20\n",
    "\n",
    "image_size = 128\n",
    "num_labels = 9\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "print(train_filenames, test_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tianchi data set details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 1 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 2 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 3 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 4 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 5 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 6 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 7 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 8 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "def print_data_details(zip_filenames):\n",
    "    \"\"\"Print the number of images and dimensions for each class.\n",
    "    \n",
    "    Args:\n",
    "        zip_filenames (zipped list): Path to pickle files containing\n",
    "            the files to process. Contain the test and train list.\n",
    "            Contain also the classes labels.\n",
    "    \"\"\"\n",
    "    for f in zip_filenames:\n",
    "        in_f_train = open(f[0], 'rb')\n",
    "        in_f_test = open(f[1], 'rb')\n",
    "        train_images = pickle.load(in_f_train)\n",
    "        test_images = pickle.load(in_f_test)\n",
    "        print('Class %d details:' % f[2])\n",
    "        print('\\tTraining data set shape', train_images.shape)\n",
    "        print('\\tTesting data set shape ', test_images.shape)\n",
    "\n",
    "print_data_details(zip(train_filenames, test_filenames, classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototyping with a fraction of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path /Volumes/Macintosh HD/Users/Benze/Downloads/data/train20 is being created.\n",
      "Path /Volumes/Macintosh HD/Users/Benze/Downloads/data/test20 is being created.\n",
      "Processing images of class 0\n",
      "\tCreating file 111.pickle.\n",
      "\tCreating file 111.pickle.\n",
      "Processing images of class 1\n",
      "\tCreating file 155.pickle.\n",
      "\tCreating file 155.pickle.\n",
      "Processing images of class 2\n",
      "\tCreating file 160.pickle.\n",
      "\tCreating file 160.pickle.\n",
      "Processing images of class 3\n",
      "\tCreating file 228.pickle.\n",
      "\tCreating file 228.pickle.\n",
      "Processing images of class 4\n",
      "\tCreating file 284.pickle.\n",
      "\tCreating file 284.pickle.\n",
      "Processing images of class 5\n",
      "\tCreating file 368.pickle.\n",
      "\tCreating file 368.pickle.\n",
      "Processing images of class 6\n",
      "\tCreating file 48.pickle.\n",
      "\tCreating file 48.pickle.\n",
      "Processing images of class 7\n",
      "\tCreating file 505.pickle.\n",
      "\tCreating file 505.pickle.\n",
      "Processing images of class 8\n",
      "\tCreating file 52.pickle.\n",
      "\tCreating file 52.pickle.\n",
      "New training directory is: /Volumes/Macintosh HD/Users/Benze/Downloads/data/train20\n",
      "New testing directory is: /Volumes/Macintosh HD/Users/Benze/Downloads/data/test20\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# For prototyping, only take 1/10 fraction, i.e.:\n",
    "#   - 1000 images per training class\n",
    "#   - 200  images per testing class\n",
    "# For prototyping, 1/10 doesn't fit the RAM... With 1/20:\n",
    "#   - 500 images per training class\n",
    "#   - 100 images per testing class\n",
    "\n",
    "def subset(zip_filenames, reduction):\n",
    "    \"\"\"Take 1/times the size of the dataset for train/test.\n",
    "    \n",
    "    Args:\n",
    "        zip_filenames (zipped list): Path to pickle files containing\n",
    "            the files to process. Contain the test and train list.\n",
    "            Contain also the classes labels.\n",
    "        reduction (int): fraction of data to keep.\n",
    "    \"\"\"\n",
    "    new_train_dir = \"/\".join([data_dir, \"\".join([\"train\", str(reduction)])])\n",
    "    new_test_dir = \"/\".join([data_dir, \"\".join([\"test\", str(reduction)])])\n",
    "\n",
    "    def check_exists(path):\n",
    "            \"\"\"Check if the path already exists. If not, create it.\n",
    "            \n",
    "            Args:\n",
    "                path: Path to test.\n",
    "            \"\"\"\n",
    "            if not os.path.exists(path):\n",
    "                print('Path %s is being created.' % path)\n",
    "                os.makedirs(path)\n",
    "    \n",
    "    check_exists(new_train_dir)\n",
    "    check_exists(new_test_dir)\n",
    "    \n",
    "    def create_subset(parent_filename, sub_filename, data_dir, cls):\n",
    "        \"\"\"Create the subset of the data of type category.\n",
    "        \n",
    "        Args:\n",
    "            parent_filename (string): filename of parent file.\n",
    "            sub_filename (string): filename to create.\n",
    "            data_dir (string): Directory where the file belongs.\n",
    "            cls (int): current class processed.\n",
    "        \"\"\"\n",
    "        file_obj = \"/\".join([data_dir, sub_filename])\n",
    "        if not os.path.exists(file_obj):\n",
    "            print('\\tCreating file %s.' % sub_filename)\n",
    "            in_f = open(parent_filename, 'rb')\n",
    "            in_images = pickle.load(in_f)\n",
    "            # Total number of images in the initial dataset\n",
    "            nb_images = in_images.shape[0]\n",
    "            # Number of images to keep\n",
    "            images_kept = nb_images / reduction\n",
    "            # Images are taken randomly\n",
    "            sub_in_images = in_images[random.sample(xrange(0, nb_images), images_kept),:,:]\n",
    "            out_f = open(file_obj, 'wb')\n",
    "            pickle.dump(sub_in_images, out_f)\n",
    "        else:\n",
    "            print('\\tFile %s already created.' % sub_filename)\n",
    "\n",
    "    for f in zip_filenames:\n",
    "        train_filename = f[0].split(\"/\")[-1]\n",
    "        test_filename = f[1].split(\"/\")[-1]\n",
    "        print('Processing images of class %d' % f[2])\n",
    "        create_subset(f[0], train_filename, new_train_dir, f[2])\n",
    "        create_subset(f[1], test_filename, new_test_dir, f[2])\n",
    "    \n",
    "    return new_train_dir, new_test_dir\n",
    "    \n",
    "new_train_dir, new_test_dir = subset(zip(train_filenames, test_filenames, classes), reduction)\n",
    "print('New training directory is: %s' % new_train_dir)\n",
    "print('New testing directory is: %s' % new_test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformating the data\n",
    "### Creating one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging datasets with reduction 20.\n",
      "Processing training datatsets.\n",
      "Generating indexes.\n",
      "Indexes generated.\n",
      "Empty data and labels matrix created.\n",
      "Processing file 111.pickle\n",
      "Enf of processing of 111.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Processing file 155.pickle\n",
      "Enf of processing of 155.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Processing file 160.pickle\n",
      "Enf of processing of 160.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Processing file 228.pickle\n",
      "Enf of processing of 228.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Processing file 284.pickle\n",
      "Enf of processing of 284.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Processing file 368.pickle\n",
      "Enf of processing of 368.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Processing file 48.pickle\n",
      "Enf of processing of 48.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Processing file 505.pickle\n",
      "Enf of processing of 505.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Writing the new data...\n",
      "Data written.\n",
      "Writing the new labels...\n",
      "Labels written.\n",
      "Time processing: -190 sec\n",
      "End of processing training datasets.\n",
      "Processing testing datatsets.\n",
      "Generating indexes.\n",
      "Indexes generated.\n",
      "Empty data and labels matrix created.\n",
      "Processing file 111.pickle\n",
      "Enf of processing of 111.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Processing file 155.pickle\n",
      "Enf of processing of 155.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Processing file 160.pickle\n",
      "Enf of processing of 160.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Processing file 228.pickle\n",
      "Enf of processing of 228.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Processing file 284.pickle\n",
      "Enf of processing of 284.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Processing file 368.pickle\n",
      "Enf of processing of 368.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Processing file 48.pickle\n",
      "Enf of processing of 48.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]]\n",
      "Processing file 505.pickle\n",
      "Enf of processing of 505.pickle\n",
      "[[ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]]\n",
      "Writing the new data...\n",
      "Data written.\n",
      "Writing the new labels...\n",
      "Labels written.\n",
      "Time processing: -38 sec\n",
      "End of processing training datasets.\n"
     ]
    }
   ],
   "source": [
    "def create_idx(nb_images, classes):\n",
    "    \"\"\"Create the indexes to shuffle the images.\n",
    "\n",
    "    Args:\n",
    "        nb_images (int): total number of images\n",
    "        classes (int list): list of classes.\n",
    "    Returns:\n",
    "        dict: indexes for each class.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    idx = list(range(nb_images))\n",
    "    random.shuffle(idx)\n",
    "    \n",
    "    # Number of images per class. With reduction=10, nb=1000\n",
    "    # Indeed, nb_images=9000 et classes=9\n",
    "    nb = nb_images / len(classes)\n",
    "    for c in classes:\n",
    "        result[c] = {}\n",
    "        for i in range(nb):\n",
    "            result[c][i] = idx[i+nb*c]\n",
    "    return result\n",
    "\n",
    "def merge_classes(data_dir, category, nb_images, classes):\n",
    "    \"\"\"Merge the files into one matrix.\n",
    "    Uesr need to know in advance how many images there are.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (string): directory of the dataset to merge.\n",
    "        category (string): category of the current data (train/test).\n",
    "        nb_images (int): total number of images.\n",
    "        classes (int list): list of the classes to process.\n",
    "    Returns:\n",
    "        ndarray: merged dataset\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    # Only is tested data.pickle. Indeed, both files are created at\n",
    "    # the same time. Impossible to have only one file.\n",
    "    if not os.path.exists('/'.join([data_dir, 'data.pickle'])):\n",
    "        print('Generating indexes.')\n",
    "        idx = create_idx(nb_images, classes)\n",
    "        print('Indexes generated.')\n",
    "        # Result data and labels ndarray that is being created.\n",
    "        data = np.zeros(shape=(nb_images, image_size, image_size))\n",
    "        labels = np.zeros(shape=(nb_images, len(classes)))\n",
    "        print('Empty data and labels matrix created.')\n",
    "\n",
    "        filenames = os.listdir(data_dir) # List all class files.\n",
    "        for f in zip(filenames, classes): # Use to retrieve the indexes.\n",
    "            if(f[0] == '.DS_Store'):\n",
    "                continue\n",
    "            print('Processing file %s' % f[0])\n",
    "            in_f = open(\"/\".join([data_dir, f[0]]), 'rb')\n",
    "            images = pickle.load(in_f)\n",
    "            for i in range(len(images)):\n",
    "                ix = idx[f[1]][i] # Retrieve the new index\n",
    "                data[ix,:,:] = images[i,:,:] # Copy the image into data\n",
    "                labels[ix,f[1]] = 1\n",
    "            print('Enf of processing of %s' % f[0])\n",
    "        print('Writing the new data...')\n",
    "        out_data_f = open('/'.join([data_dir, 'data.pickle']), 'wb')\n",
    "        pickle.dump(data, out_data_f)\n",
    "        out_data_f.close()\n",
    "        print('Data written.')\n",
    "        print('Writing the new labels...')\n",
    "        out_labels_f = open('/'.join([data_dir, 'labels.pickle']), 'wb')\n",
    "        pickle.dump(labels, out_labels_f)\n",
    "        out_labels_f.close()\n",
    "        print('Labels written.')\n",
    "        print('Time processing: %d sec' % (t0-time.time()))\n",
    "    else: # Files have already been created.\n",
    "        print('Files have already been created.')\n",
    "        print('Time processing: %d sec' % (t0-time.time()))\n",
    "        \n",
    "print('Merging datasets with reduction %d.' % reduction)\n",
    "print('Processing training datatsets.')\n",
    "merge_classes(new_train_dir, \"train\", nb_total_train_img/reduction, classes)\n",
    "print('End of processing training datasets.')\n",
    "print('Processing testing datatsets.')\n",
    "merge_classes(new_test_dir, \"test\", nb_total_test_img/reduction, classes)\n",
    "print('End of processing test datasets.')\n",
    "\n",
    "# Till here, there is nothing very big in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing data...\n",
      "Testing data/labels loaded in 52 sec\n",
      "Loading training data...\n",
      "Training data/labels loaded in 270 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\"Load the data in the data_dir.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (string): data directory to load.\n",
    "    Returns:\n",
    "        dataset: dataset for train/test.\n",
    "        labels: labels for train/test.\n",
    "    \"\"\"\n",
    "    def load(path):\n",
    "        \"\"\"Load pickled file.\n",
    "        \n",
    "        Args:\n",
    "            path (string): path of the pickle file.\n",
    "        Returns:\n",
    "            data (ndarray): pickled file.\n",
    "        \"\"\"\n",
    "        f = open(path, 'rb')\n",
    "        return pickle.load(f)\n",
    "    \n",
    "    data = load('/'.join([data_dir, 'data.pickle']))\n",
    "    labels = load('/'.join([data_dir, 'labels.pickle']))\n",
    "    return data, labels\n",
    "    \n",
    "def reformat(dataset, labels):\n",
    "    \"\"\"Format the ndarray to fit into a tensorflow Tensor.\n",
    "    \n",
    "    Args:\n",
    "        dataset (numpy ndarray): Input data set.\n",
    "        labels (numpy ndarray): Input data labels.\n",
    "    Returns:\n",
    "        ndarray: formatted dataset.\n",
    "        ndarray: formatted labels.\n",
    "    \"\"\"\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "#     labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "# Has the test_data already being created?\n",
    "if not ('test_data' in vars() and 'test_labels' in vars()):\n",
    "    # Load the data in memory\n",
    "    t0 = time.time()\n",
    "    print('Loading testing data...')\n",
    "    test_data, test_labels = load_data(new_test_dir)\n",
    "    print('Testing data/labels loaded in %d sec' % (time.time()-t0))\n",
    "else:\n",
    "    print('Testing data/labels is already loaded.')\n",
    "\n",
    "if not (('train_data' in vars()) and ('train_labels' in vars())):\n",
    "    t0 = time.time()\n",
    "    print('Loading training data...')\n",
    "    train_data, train_labels = load_data(new_train_dir)\n",
    "    print('Training data/labels loaded in %d sec' % (time.time()-t0))\n",
    "else:\n",
    "    print('Training data/labels is alreayd loaded.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting testing data...\n",
      "Testing data/labels formatted in 0 sec\n",
      "Formatting training data...\n",
      "Training data/labels formatted in 0 sec\n",
      "Training set dataset (4500, 128, 128, 1) labels (4500, 9)\n",
      "Testing set dataset (900, 128, 128, 1) labels (900, 9)\n"
     ]
    }
   ],
   "source": [
    "# Train_labels are 1 hot encoded\n",
    "t0 = time.time()\n",
    "print('Formatting testing data...')\n",
    "test_data, test_labels = reformat(test_data, test_labels)\n",
    "print('Testing data/labels formatted in %d sec' % (time.time()-t0))\n",
    "t0 = time.time()\n",
    "print('Formatting training data...')\n",
    "train_data, train_labels = reformat(train_data, train_labels)\n",
    "print('Training data/labels formatted in %d sec' % (time.time()-t0))\n",
    "\n",
    "print('Training set dataset', train_data.shape, 'labels', train_labels.shape)\n",
    "print('Testing set dataset', test_data.shape, 'labels', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the accuracy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "### Modeling\n",
    "\n",
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "def model(data):\n",
    "    \"\"\"Create the flow of the model.\n",
    "    \n",
    "    Args:\n",
    "        data (4-D Tensor): training data.\n",
    "    Returns:\n",
    "        4D-Tensor: result of the prediction for each class.\n",
    "    \"\"\"\n",
    "    # 1st vanilla convolution with padding.\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    # 1st ReLU output.\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    # 2nd vanilla convolution with padding.\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    # 2nd ReLU output.\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    # Reshaping the hidden output.\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    # Fully connected neural network.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "# Seting the data, weights and biases variables within the default Graph.\n",
    "# Use tf.trainable_variables() gives the trainable variables currently in\n",
    "# the graph that are used by the optimizer. Need to catch everything in the\n",
    "# environment.\n",
    "with graph.as_default():\n",
    "    # Training data placeholder.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels),\n",
    "        name='tf_train_dataset')\n",
    "    # Training labels placeholder.\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels),\n",
    "        name='tf_train_labels')\n",
    "    # Testing data placeholder.\n",
    "    tf_test_dataset = tf.constant(test_data,\n",
    "        name='tf_test_dataset')\n",
    "\n",
    "    # 1st convolutional layer Weight and Biases.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, num_channels, depth], stddev=0.1),\n",
    "            name='layer1_weights', trainable=True)\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]),\n",
    "            name='layer1_biases', trainable=True)\n",
    "    # 2nd convolutional layer Weight and Biases.\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, depth, depth], stddev=0.1),\n",
    "            name='layer2_weights', trainable=True)\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]),\n",
    "            name='layer2_biases', trainable=True)\n",
    "    # 3rd layer Weight and Biases.\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "            [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1),\n",
    "            name='layer3_weights', trainable=True)\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]),\n",
    "            name='layer3_biases', trainable=True)\n",
    "    # 4th layer Weight and Biases.\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [num_hidden, num_labels], stddev=0.1),\n",
    "            name='layer4_weights', trainable=True)\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]),\n",
    "            name='layer4_biases', trainable=True)\n",
    "    # Connecting TensorFlow Ops.\n",
    "    logits = model(tf_train_dataset)\n",
    "\n",
    "    # Following model uses the cross-entropy model.\n",
    "    loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels, name='cross_entropy'),\n",
    "                name='loss')\n",
    "\n",
    "    # Optmizer tensor.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05, use_locking=False,\n",
    "                                                  name='gradient_descent').minimize(loss,\\\n",
    "                               var_list=[layer1_weights, layer1_biases,\n",
    "                                         layer2_weights, layer2_biases,\n",
    "                                         layer3_weights, layer3_biases,\n",
    "                                         layer4_weights, layer4_biases])\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "\n",
    "    # Predictions for the training, validation (not yet implemented), and test data.\n",
    "    train_prediction = tf.nn.softmax(logits, name='train_prediction')\n",
    "    # valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset), name='test_prediction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "(4500, 9)\n",
      "Minibatch loss at step 0: 6.147752\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 2: 9.235579\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 4: 5.502340\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 6: 2.862674\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 8: 2.304332\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 10: 2.119406\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 12: 1.763381\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 14: 2.320365\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 16: 2.204549\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 18: 2.590378\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 20: 2.084889\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 22: 2.010514\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 24: 1.645474\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 26: 2.210882\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 28: 2.437295\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 30: 2.060897\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 32: 1.997741\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 34: 1.698071\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 36: 2.030350\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 38: 1.786363\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 40: 2.047691\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 42: 1.780360\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 44: 2.002967\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 46: 2.038327\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 48: 2.067583\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 50: 1.895261\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 52: 2.201626\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 54: 2.509663\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 56: 1.441522\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 58: 1.884920\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 60: 2.339431\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 62: 3.368746\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 64: 1.986889\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 66: 1.936773\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 68: 1.681587\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 70: 1.700794\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 72: 2.057010\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 74: 2.070935\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 76: 1.965548\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 78: 2.119327\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 80: 2.182955\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 82: 2.049800\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 84: 2.044852\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 86: 1.499484\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 88: 1.777718\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 90: 2.005997\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 92: 2.102476\n",
      "Minibatch accuracy: 12.5%\n",
      "Minibatch loss at step 94: 1.991325\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 96: 1.333039\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 98: 2.124059\n",
      "Minibatch accuracy: 6.2%\n",
      "Test accuracy: 11.4%\n"
     ]
    }
   ],
   "source": [
    "# NB: before launching the model, need to restart the network.\n",
    "\n",
    "num_steps = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    print(train_labels.shape)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 2 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "#           print('Validation accuracy: %.1f%%' % accuracy(\n",
    "#                 valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), _r_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
