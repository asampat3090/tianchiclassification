{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tianchi Notebook. Clothes classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/train/train-file-0.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/train/train-file-1.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/train/train-file-2.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/train/train-file-3.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/train/train-file-4.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/train/train-file-5.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/train/train-file-6.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/train/train-file-7.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/train/train-file-8.pickle'] ['/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/test//test-file-0.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/test//test-file-1.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/test//test-file-2.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/test//test-file-3.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/test//test-file-4.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/test//test-file-5.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/test//test-file-6.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/test//test-file-7.pickle', '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/test//test-file-8.pickle']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '/Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi'\n",
    "train_data_path = \"/\".join([data_dir, 'train'])\n",
    "test_data_path = \"/\".join([data_dir, 'test/'])\n",
    "\n",
    "# train ~ 6Gb and test ~ 1.2 Gb\n",
    "train_filenames = [\"/\".join([train_data_path, f]) for f in os.listdir(train_data_path)]\n",
    "test_filenames = [\"/\".join([test_data_path, f]) for f in os.listdir(test_data_path)]\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "nb_total_train_img = 90000\n",
    "nb_total_test_img = 18000\n",
    "\n",
    "reduction = 20\n",
    "\n",
    "image_size = 128\n",
    "num_labels = 9\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "print(train_filenames, test_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tianchi data set details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 1 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 2 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 3 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 4 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 5 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 6 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 7 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n",
      "Class 8 details:\n",
      "\tTraining data set shape (10000, 128, 128)\n",
      "\tTesting data set shape  (2000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "def print_data_details(zip_filenames):\n",
    "    \"\"\"Print the number of images and dimensions for each class.\n",
    "    \n",
    "    Args:\n",
    "        zip_filenames (zipped list): Path to pickle files containing\n",
    "            the files to process. Contain the test and train list.\n",
    "            Contain also the classes labels.\n",
    "    \"\"\"\n",
    "    for f in zip_filenames:\n",
    "        in_f_train = open(f[0], 'rb')\n",
    "        in_f_test = open(f[1], 'rb')\n",
    "        train_images = pickle.load(in_f_train)\n",
    "        test_images = pickle.load(in_f_test)\n",
    "        print('Class %d details:' % f[2])\n",
    "        print('\\tTraining data set shape', train_images.shape)\n",
    "        print('\\tTesting data set shape ', test_images.shape)\n",
    "\n",
    "print_data_details(zip(train_filenames, test_filenames, classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototyping with a fraction of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images of class 0\n",
      "\tFile train-file-0.pickle already created.\n",
      "\tFile test-file-0.pickle already created.\n",
      "Processing images of class 1\n",
      "\tFile train-file-1.pickle already created.\n",
      "\tFile test-file-1.pickle already created.\n",
      "Processing images of class 2\n",
      "\tFile train-file-2.pickle already created.\n",
      "\tFile test-file-2.pickle already created.\n",
      "Processing images of class 3\n",
      "\tFile train-file-3.pickle already created.\n",
      "\tFile test-file-3.pickle already created.\n",
      "Processing images of class 4\n",
      "\tFile train-file-4.pickle already created.\n",
      "\tFile test-file-4.pickle already created.\n",
      "Processing images of class 5\n",
      "\tFile train-file-5.pickle already created.\n",
      "\tFile test-file-5.pickle already created.\n",
      "Processing images of class 6\n",
      "\tFile train-file-6.pickle already created.\n",
      "\tFile test-file-6.pickle already created.\n",
      "Processing images of class 7\n",
      "\tFile train-file-7.pickle already created.\n",
      "\tFile test-file-7.pickle already created.\n",
      "Processing images of class 8\n",
      "\tFile train-file-8.pickle already created.\n",
      "\tFile test-file-8.pickle already created.\n",
      "New training directory is: /Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/train20\n",
      "New testing directory is: /Volumes/Macintosh HD/Users/perezmunoz/Data/tianchi/test20\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# For prototyping, only take 1/10 fraction, i.e.:\n",
    "#   - 1000 images per training class\n",
    "#   - 200  images per testing class\n",
    "# For prototyping, 1/10 doesn't fit the RAM... With 1/20:\n",
    "#   - 500 images per training class\n",
    "#   - 100 images per testing class\n",
    "\n",
    "def subset(zip_filenames, reduction):\n",
    "    \"\"\"Take 1/times the size of the dataset for train/test.\n",
    "    \n",
    "    Args:\n",
    "        zip_filenames (zipped list): Path to pickle files containing\n",
    "            the files to process. Contain the test and train list.\n",
    "            Contain also the classes labels.\n",
    "        reduction (int): fraction of data to keep.\n",
    "    \"\"\"\n",
    "    new_train_dir = \"/\".join([data_dir, \"\".join([\"train\", str(reduction)])])\n",
    "    new_test_dir = \"/\".join([data_dir, \"\".join([\"test\", str(reduction)])])\n",
    "\n",
    "    def check_exists(path):\n",
    "            \"\"\"Check if the path already exists. If not, create it.\n",
    "            \n",
    "            Args:\n",
    "                path: Path to test.\n",
    "            \"\"\"\n",
    "            if not os.path.exists(path):\n",
    "                print('Path %s is being created.' % path)\n",
    "                os.makedirs(path)\n",
    "    \n",
    "    check_exists(new_train_dir)\n",
    "    check_exists(new_test_dir)\n",
    "    \n",
    "    def create_subset(parent_filename, sub_filename, data_dir, cls):\n",
    "        \"\"\"Create the subset of the data of type category.\n",
    "        \n",
    "        Args:\n",
    "            parent_filename (string): filename of parent file.\n",
    "            sub_filename (string): filename to create.\n",
    "            data_dir (string): Directory where the file belongs.\n",
    "            cls (int): current class processed.\n",
    "        \"\"\"\n",
    "        file_obj = \"/\".join([data_dir, sub_filename])\n",
    "        if not os.path.exists(file_obj):\n",
    "            print('\\tCreating file %s.' % sub_filename)\n",
    "            in_f = open(parent_filename, 'rb')\n",
    "            in_images = pickle.load(in_f)\n",
    "            # Total number of images in the initial dataset\n",
    "            nb_images = in_images.shape[0]\n",
    "            # Number of images to keep\n",
    "            images_kept = nb_images / reduction\n",
    "            # Images are taken randomly\n",
    "            sub_in_images = in_images[random.sample(xrange(0, nb_images), images_kept),:,:]\n",
    "            out_f = open(file_obj, 'wb')\n",
    "            pickle.dump(sub_in_images, out_f)\n",
    "        else:\n",
    "            print('\\tFile %s already created.' % sub_filename)\n",
    "\n",
    "    for f in zip_filenames:\n",
    "        train_filename = f[0].split(\"/\")[-1]\n",
    "        test_filename = f[1].split(\"/\")[-1]\n",
    "        print('Processing images of class %d' % f[2])\n",
    "        create_subset(f[0], train_filename, new_train_dir, f[2])\n",
    "        create_subset(f[1], test_filename, new_test_dir, f[2])\n",
    "    \n",
    "    return new_train_dir, new_test_dir\n",
    "    \n",
    "new_train_dir, new_test_dir = subset(zip(train_filenames, test_filenames, classes), reduction)\n",
    "print('New training directory is: %s' % new_train_dir)\n",
    "print('New testing directory is: %s' % new_test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformating the data\n",
    "### Creating one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging datasets with reduction 20.\n",
      "Processing training datatsets.\n",
      "Generating indexes.\n",
      "Indexes generated.\n",
      "Empty data and labels matrix created.\n",
      "Processing file train-file-0.pickle\n",
      "Enf of processing of train-file-0.pickle\n",
      "Processing file train-file-1.pickle\n",
      "Enf of processing of train-file-1.pickle\n",
      "Processing file train-file-2.pickle\n",
      "Enf of processing of train-file-2.pickle\n",
      "Processing file train-file-3.pickle\n",
      "Enf of processing of train-file-3.pickle\n",
      "Processing file train-file-4.pickle\n",
      "Enf of processing of train-file-4.pickle\n",
      "Processing file train-file-5.pickle\n",
      "Enf of processing of train-file-5.pickle\n",
      "Processing file train-file-6.pickle\n",
      "Enf of processing of train-file-6.pickle\n",
      "Processing file train-file-7.pickle\n",
      "Enf of processing of train-file-7.pickle\n",
      "Processing file train-file-8.pickle\n",
      "Enf of processing of train-file-8.pickle\n",
      "Writing the new data...\n",
      "Data written.\n",
      "Writing the new labels...\n",
      "Labels written.\n",
      "Time processing: -216 sec\n",
      "End of processing training datasets.\n",
      "Processing testing datatsets.\n",
      "Generating indexes.\n",
      "Indexes generated.\n",
      "Empty data and labels matrix created.\n",
      "Processing file test-file-0.pickle\n",
      "Enf of processing of test-file-0.pickle\n",
      "Processing file test-file-1.pickle\n",
      "Enf of processing of test-file-1.pickle\n",
      "Processing file test-file-2.pickle\n",
      "Enf of processing of test-file-2.pickle\n",
      "Processing file test-file-3.pickle\n",
      "Enf of processing of test-file-3.pickle\n",
      "Processing file test-file-4.pickle\n",
      "Enf of processing of test-file-4.pickle\n",
      "Processing file test-file-5.pickle\n",
      "Enf of processing of test-file-5.pickle\n",
      "Processing file test-file-6.pickle\n",
      "Enf of processing of test-file-6.pickle\n",
      "Processing file test-file-7.pickle\n",
      "Enf of processing of test-file-7.pickle\n",
      "Processing file test-file-8.pickle\n",
      "Enf of processing of test-file-8.pickle\n",
      "Writing the new data...\n",
      "Data written.\n",
      "Writing the new labels...\n",
      "Labels written.\n",
      "Time processing: -44 sec\n",
      "End of processing training datasets.\n"
     ]
    }
   ],
   "source": [
    "def create_idx(nb_images, classes):\n",
    "    \"\"\"Create the indexes to shuffle the images.\n",
    "\n",
    "    Args:\n",
    "        nb_images (int): total number of images\n",
    "        classes (int list): list of classes.\n",
    "    Returns:\n",
    "        dict: indexes for each class.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    idx = random.sample(xrange(0,nb_images), nb_images)\n",
    "    \n",
    "    # Number of images per class. With reduction=10, nb=1000\n",
    "    # Indeed, nb_images=9000 et classes=9\n",
    "    nb = nb_images / len(classes)\n",
    "    for c in classes:\n",
    "        result[c] = {}\n",
    "        for i in range(nb):\n",
    "            result[c][i] = idx[i+nb*c]\n",
    "    return result\n",
    "\n",
    "def merge_classes(data_dir, category, nb_images, classes):\n",
    "    \"\"\"Merge the files into one matrix.\n",
    "    Uesr need to know in advance how many images there are.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (string): directory of the dataset to merge.\n",
    "        category (string): category of the current data (train/test).\n",
    "        nb_images (int): total number of images.\n",
    "        classes (int list): list of the classes to process.\n",
    "    Returns:\n",
    "        ndarray: merged dataset\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    # Only is tested data.pickle. Indeed, both files are created at\n",
    "    # the same time. Impossible to have only one file.\n",
    "    if not os.path.exists('/'.join([data_dir, 'data.pickle'])):\n",
    "        print('Generating indexes.')\n",
    "        idx = create_idx(nb_images, classes)\n",
    "        print('Indexes generated.')\n",
    "        # Result data and labels ndarray that is being created.\n",
    "        data = np.zeros(shape=(nb_images, image_size, image_size))\n",
    "        labels = np.zeros(shape=(nb_images, len(classes)))\n",
    "        print('Empty data and labels matrix created.')\n",
    "\n",
    "        filenames = os.listdir(data_dir) # List all class files.\n",
    "        for f in zip(filenames, classes): # Use to retrieve the indexes.\n",
    "            print('Processing file %s' % f[0])\n",
    "            in_f = open(\"/\".join([data_dir, f[0]]), 'rb')\n",
    "            images = pickle.load(in_f)\n",
    "            for i in range(len(images)):\n",
    "                ix = idx[f[1]][i] # Retrieve the new index\n",
    "                data[ix,:,:] = images[i,:,:] # Copy the image into data\n",
    "                labels[ix,f[1]] = 1\n",
    "            print('Enf of processing of %s' % f[0])\n",
    "        print('Writing the new data...')\n",
    "        out_data_f = open('/'.join([data_dir, 'data.pickle']), 'wb')\n",
    "        pickle.dump(data, out_data_f)\n",
    "        out_data_f.close()\n",
    "        print('Data written.')\n",
    "        print('Writing the new labels...')\n",
    "        out_labels_f = open('/'.join([data_dir, 'labels.pickle']), 'wb')\n",
    "        pickle.dump(labels, out_labels_f)\n",
    "        out_labels_f.close()\n",
    "        print('Labels written.')\n",
    "        print('Time processing: %d sec' % (t0-time.time()))\n",
    "    else: # Files have already been created.\n",
    "        print('Files have already been created.')\n",
    "        print('Time processing: %d sec' % (t0-time.time()))\n",
    "        \n",
    "print('Merging datasets with reduction %d.' % reduction)\n",
    "print('Processing training datatsets.')\n",
    "merge_classes(new_train_dir, \"train\", nb_total_train_img/reduction, classes)\n",
    "print('End of processing training datasets.')\n",
    "print('Processing testing datatsets.')\n",
    "merge_classes(new_test_dir, \"test\", nb_total_test_img/reduction, classes)\n",
    "print('End of processing training datasets.')\n",
    "\n",
    "# Till here, there is nothing very big in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading testing data...\n",
      "Testing data/labels loaded in 42 sec\n",
      "Loading training data...\n",
      "Training data/labels loaded in 243 sec\n",
      "Formatting testing data...\n",
      "(900, 1, 9)\n",
      "Testing data/labels formatted in 0 sec\n",
      "Formatting training data...\n",
      "(4500, 1, 9)\n",
      "Training data/labels formatted in 0 sec\n",
      "Training set dataset (4500, 128, 128, 1) labels (4500, 1, 9)\n",
      "Testing set dataset (900, 128, 128, 1) labels (900, 1, 9)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\"Load the data in the data_dir.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (string): data directory to load.\n",
    "    Returns:\n",
    "        dataset: dataset for train/test.\n",
    "        labels: labels for train/test.\n",
    "    \"\"\"\n",
    "    def load(path):\n",
    "        \"\"\"Load pickled file.\n",
    "        \n",
    "        Args:\n",
    "            path (string): path of the pickle file.\n",
    "        Returns:\n",
    "            data (ndarray): pickled file.\n",
    "        \"\"\"\n",
    "        f = open(path, 'rb')\n",
    "        return pickle.load(f)\n",
    "    \n",
    "    data = load('/'.join([data_dir, 'data.pickle']))\n",
    "    labels = load('/'.join([data_dir, 'labels.pickle']))\n",
    "    return data, labels\n",
    "    \n",
    "def reformat(dataset, labels):\n",
    "    \"\"\"Format the ndarray to fit into a tensorflow Tensor.\n",
    "    \n",
    "    Args:\n",
    "        dataset (numpy ndarray): Input data set.\n",
    "        labels (numpy ndarray): Input data labels.\n",
    "    Returns:\n",
    "        ndarray: formatted dataset.\n",
    "        ndarray: formatted labels.\n",
    "    \"\"\"\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    print(labels.shape)\n",
    "    return dataset, labels\n",
    "\n",
    "# Has the test_data already being created?\n",
    "if not ('test_data' in vars() and 'test_labels' in vars()):\n",
    "    # Load the data in memory\n",
    "    t0 = time.time()\n",
    "    print('Loading testing data...')\n",
    "    test_data, test_labels = load_data(new_test_dir)\n",
    "    print('Testing data/labels loaded in %d sec' % (time.time()-t0))\n",
    "else:\n",
    "    print('Testing data/labels is already loaded.')\n",
    "\n",
    "if not (('train_data' in vars()) and ('labels_data' in vars())):\n",
    "    t0 = time.time()\n",
    "    print('Loading training data...')\n",
    "    train_data, train_labels = load_data(new_train_dir)\n",
    "    print('Training data/labels loaded in %d sec' % (time.time()-t0))\n",
    "else:\n",
    "    print('Training data/labels is alreayd loaded.')\n",
    "\n",
    "# Train_labels are 1 hot encoded\n",
    "t0 = time.time()\n",
    "print('Formatting testing data...')\n",
    "test_data, test_labels = reformat(test_data, test_labels)\n",
    "print('Testing data/labels formatted in %d sec' % (time.time()-t0))\n",
    "t0 = time.time()\n",
    "print('Formatting training data...')\n",
    "train_data, train_labels = reformat(train_data, train_labels)\n",
    "print('Training data/labels formatted in %d sec' % (time.time()-t0))\n",
    "\n",
    "print('Training set dataset', train_data.shape, 'labels', train_labels.shape)\n",
    "print('Testing set dataset', test_data.shape, 'labels', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the accuracy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "### Modeling\n",
    "\n",
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "def model(data):\n",
    "    \"\"\"Create the flow of the model.\n",
    "    \n",
    "    Args:\n",
    "        data (4-D Tensor): training data.\n",
    "    Returns:\n",
    "        4D-Tensor: result of the prediction for each class.\n",
    "    \"\"\"\n",
    "    # 1st vanilla convolution with padding.\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    # 1st ReLU output.\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    # 2nd vanilla convolution with padding.\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    # 2nd ReLU output.\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    # Reshaping the hidden output.\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    # Fully connected neural network.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "# Seting the data, weights and biases variables within the default Graph.\n",
    "# Use tf.trainable_variables() gives the trainable variables currently in\n",
    "# the graph that are used by the optimizer. Need to catch everything in the\n",
    "# environment.\n",
    "with graph.as_default():\n",
    "    # Training data placeholder.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels),\n",
    "        name='tf_train_dataset')\n",
    "    # Training labels placeholder.\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels),\n",
    "        name='tf_train_labels')\n",
    "    # Testing data placeholder.\n",
    "    tf_test_dataset = tf.constant(test_data,\n",
    "        name='tf_test_dataset')\n",
    "\n",
    "    # 1st convolutional layer Weight and Biases.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, num_channels, depth], stddev=0.1),\n",
    "            name='layer1_weights', trainable=True)\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]),\n",
    "            name='layer1_biases', trainable=True)\n",
    "    # 2nd convolutional layer Weight and Biases.\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, depth, depth], stddev=0.1),\n",
    "            name='layer2_weights', trainable=True)\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]),\n",
    "            name='layer2_biases', trainable=True)\n",
    "    # 3rd layer Weight and Biases.\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "            [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1),\n",
    "            name='layer3_weights', trainable=True)\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]),\n",
    "            name='layer3_biases', trainable=True)\n",
    "    # 4th layer Weight and Biases.\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [num_hidden, num_labels], stddev=0.1),\n",
    "            name='layer4_weights', trainable=True)\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]),\n",
    "            name='layer4_biases', trainable=True)\n",
    "    # Connecting TensorFlow Ops.\n",
    "    logits = model(tf_train_dataset)\n",
    "\n",
    "    # Following model uses the cross-entropy model.\n",
    "    loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels, name='cross_entropy'),\n",
    "                name='loss')\n",
    "\n",
    "    # Optmizer tensor.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05, use_locking=False,\n",
    "                                                  name='gradient_descent').minimize(loss,\\\n",
    "                               var_list=[layer1_weights, layer1_biases,\n",
    "                                         layer2_weights, layer2_biases,\n",
    "                                         layer3_weights, layer3_biases,\n",
    "                                         layer4_weights, layer4_biases])\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "\n",
    "    # Predictions for the training, validation (not yet implemented), and test data.\n",
    "    train_prediction = tf.nn.softmax(logits, name='train_prediction')\n",
    "    # valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset), name='test_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 7.941812\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2: 104.469597\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4: 0.668412\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 6: 0.237376\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8: 0.237362\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 10: 0.128968\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12: 0.083489\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 14: 0.704166\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16: 0.618293\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 18: 0.239627\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20: 0.055128\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 22: 0.538022\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 24: 0.375200\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 26: 0.319509\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 28: 0.253676\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 30: 0.381619\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 32: 0.529156\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 34: 0.242027\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 36: 0.062200\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 38: 0.543119\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 40: 0.645067\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 42: 0.230476\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 44: 0.081726\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 46: 0.753762\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 48: 0.230444\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 50: 0.245636\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 52: 0.361364\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 54: 0.480777\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 56: 0.387103\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 58: 0.247174\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 60: 0.516984\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 62: 0.117426\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 64: 0.382188\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 66: 0.062325\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 68: 0.097302\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 70: 0.389970\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 72: 0.132303\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 74: 0.243200\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 76: 0.626342\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 78: 0.307908\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 80: 0.484871\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 82: 0.540031\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 84: 0.064363\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 86: 0.382950\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 88: 0.306498\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 90: 0.370088\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 92: 0.769825\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 94: 0.552214\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 96: 0.790907\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 98: 0.297832\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/perezmunoz/Documents/Envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:3: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# NB: before launching the model, need to restart the network.\n",
    "\n",
    "num_steps = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), 0, :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 2 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "#           print('Validation accuracy: %.1f%%' % accuracy(\n",
    "#                 valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
